{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn\n",
      "  Downloading hmmlearn-0.2.6-cp38-cp38-win_amd64.whl (118 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from hmmlearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from hmmlearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from hmmlearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jiangian\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python[pandas] in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: pyOpenSSL<21.0.0,>=16.2.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (19.1.0)\n",
      "Requirement already satisfied: cryptography<4.0.0,>=3.1.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (3.1.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.0.1)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (3.10.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.14.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages (from snowflake-connector-python[pandas]) (2021.1)\n",
      "Requirement already satisfied: setuptools>34.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (50.3.1.post20201107)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.2.1)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.24.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2020.12.5)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.4.0)\n",
      "Requirement already satisfied: pyarrow<5.1.0,>=5.0.0 in c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages (from snowflake-connector-python[pandas]) (5.0.0)\n",
      "Requirement already satisfied: pandas<1.4.0,>=1.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.20)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from cryptography<4.0.0,>=3.1.0->snowflake-connector-python[pandas]) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages (from pandas<1.4.0,>=1.0.0->snowflake-connector-python[pandas]) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from pandas<1.4.0,>=1.0.0->snowflake-connector-python[pandas]) (1.19.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (1.25.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jiangian\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user \"snowflake-connector-python[pandas]\" --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import indicators as ind\n",
    "import time\n",
    "from kneed import DataGenerator, KneeLocator\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import rankdata\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# from stock.technical import get_all_indices\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "from pycaret.classification import *\n",
    "\n",
    "\n",
    "#===========================================================\n",
    "#====================FEATURE ENGINEERING====================\n",
    "#===========================================================\n",
    "\n",
    "def clusterize(df, cluster_num=2):\n",
    "    def find_cluster_num(data, max_clusters=10):\n",
    "        sse = {}\n",
    "        for k in range(1, max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=k)\n",
    "            kmeans.fit(data)\n",
    "            sse[k] = kmeans.inertia_\n",
    "        kn = KneeLocator(x=list(sse.keys()), \n",
    "                  y=list(sse.values()), \n",
    "                  curve='convex', \n",
    "                  direction='decreasing')\n",
    "        return kn.knee \n",
    "    df_out= df.copy()\n",
    "    clust_model_dict = {}\n",
    "    order_dict = {}\n",
    "    for col in tqdm(df.columns):\n",
    "        col_output = df[col]\n",
    "        col_output = col_output.replace([np.inf, -np.inf], np.nan)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            if cluster_num is None:\n",
    "                cluster_num_tmp = find_cluster_num(col_output.dropna().values.reshape(-1, 1))\n",
    "            else:\n",
    "                cluster_num_tmp = cluster_num\n",
    "            kmeans = KMeans(n_clusters=cluster_num_tmp).fit(col_output.dropna().values.reshape(-1, 1))\n",
    "            clust_model_dict[col] = kmeans\n",
    "            order_map = dict(zip(range(cluster_num_tmp), np.squeeze(rankdata(kmeans.cluster_centers_)).tolist()))\n",
    "            col_output[~col_output.isna()] = list(map(lambda x: order_map[x], kmeans.labels_))\n",
    "            order_dict[col] = order_map\n",
    "        df_out[col] = col_output\n",
    "    return df_out, clust_model_dict, order_dict\n",
    "\n",
    "\n",
    "def get_all_indices(data_final, rolling_periods, groupby_col=\"Day\", \\\n",
    "                    metrics=[\"std\", \"mean\", \"min\", \"max\", \"skew\", \"pct_change\"], col_dict=None):\n",
    "    if col_dict is None:\n",
    "        col_dict = {m:data_final.columns.tolist() for m in metrics}\n",
    "    data_out = data_final.copy()\n",
    "    for metric in metrics:\n",
    "        if metric in [\"beta\", \"direct_mov\", \"cross\", \"rsi_bb\", \"sp_coin\"]:\n",
    "            for col in col_dict[metric]:\n",
    "                tmp = getattr(ind, metric)(col, data_final[col_dict[metric]])\n",
    "                tmp.columns = [col + \"_\" + c for c in tmp.columns]\n",
    "                tmp.index = data_final.index\n",
    "                data_out = data_out.join(tmp)\n",
    "        else:\n",
    "            for period in rolling_periods:\n",
    "                if metric in [\"pct_change\", \"diff\"]:\n",
    "                    if groupby_col is not None:\n",
    "                        tmp = getattr(data_final[col_dict[metric]].groupby(groupby_col), metric)(periods=period).droplevel(0)\n",
    "                    else:\n",
    "                        tmp = getattr(data_final[col_dict[metric]], metric)(periods=period)\n",
    "                else:\n",
    "                    if groupby_col is not None:\n",
    "                        tmp = getattr(data_final[col_dict[metric]].groupby(groupby_col).rolling(period, min_periods=1), metric)().droplevel(0)\n",
    "                    else:\n",
    "                        tmp = getattr(data_final[col_dict[metric]].rolling(period, min_periods=1), metric)()\n",
    "                data_out = data_out.join(tmp, lsuffix=\"\", rsuffix=\"_{}_{}\".format(metric, period))\n",
    "    return data_out\n",
    "\n",
    "\n",
    "#===============================================\n",
    "#====================PYCARET====================\n",
    "#===============================================\n",
    "\n",
    "def pycaret_automl(crypto_train, crypto_test, coin, return_period, \n",
    "                   plot=True, download=False, save_folder=\"/content/drive/MyDrive/crypto_models\",\n",
    "                  pycaret_args=None):\n",
    "    # Setting up environment\n",
    "    # CRITICAL: Out-of-time validation scheme\n",
    "    if pycaret_args is None:\n",
    "        exp_default = setup(data=crypto_train, test_data=crypto_test, target=\"target\", log_experiment=True, \n",
    "                            experiment_name=\"{}_{}\".format(coin, return_period), silent=True)\n",
    "    else:\n",
    "        exp_default = setup(data=crypto_train, test_data=crypto_test, target=\"target\", log_experiment=True, \n",
    "                    experiment_name=\"{}_{}\".format(coin, return_period), silent=True, **pycaret_args)\n",
    "    if save_folder is not None:\n",
    "        if os.path.exists(save_folder):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_models = compare_models(n_select=5, sort=\"AUC\", exclude=[\"gbc\"])\n",
    "    print(\"Completed in {} seconds\".format(time.time() - start_time))\n",
    "\n",
    "#     for mdl in best_models:\n",
    "#         mld_name = \"{}_{}_{}\".format(coin, mdl.__class__.__name__, return_period)\n",
    "#         if save_folder is not None:\n",
    "#             if save_folder.endswith(\"/\"):\n",
    "#                 save_model(mdl, save_folder + mld_name)\n",
    "#             else:\n",
    "#                 save_model(mdl, save_folder + \"/\" + mld_name)\n",
    "#         else:\n",
    "#             save_model(mdl, mld_name)\n",
    "    \n",
    "    if download:\n",
    "        files.download(mld_name + \".pkl\")    \n",
    "  \n",
    "    best_model = best_models[0]# Selecting best model\n",
    "    best_models_blend = blend_models(best_models, fold=5)\n",
    "\n",
    "    if plot:\n",
    "        print(\"========================================================================================================================================\")\n",
    "        print(\"===============================================================BEST MODEL===============================================================\")\n",
    "        print(\"========================================================================================================================================\")\n",
    "        plot_model(best_model)\n",
    "        plot_model(best_model, plot=\"confusion_matrix\")\n",
    "\n",
    "        print(\"========================================================================================================================================\")\n",
    "        print(\"===============================================================BEST BLEND===============================================================\")\n",
    "        print(\"========================================================================================================================================\")\n",
    "        plot_model(best_models_blend)\n",
    "        plot_model(best_models_blend, plot=\"confusion_matrix\")\n",
    "      \n",
    "    # Finalizing best model(s)\n",
    "    best_model_finalized = finalize_model(best_model)\n",
    "    best_blend_finalized = finalize_model(best_models_blend)\n",
    "  \n",
    "    if save_folder is not None:\n",
    "        if save_folder.endswith(\"/\"):\n",
    "            get_logs().to_csv(save_folder + \"{}_{}_log.csv\".format(coin, return_period))\n",
    "            save_model(best_model_finalized, save_folder + \"best_model_finalized\")\n",
    "            save_model(best_model_finalized, save_folder + \"best_blend_finalized\")\n",
    "        else:\n",
    "            get_logs().to_csv(save_folder + \"/{}_{}_log.csv\".format(coin, return_period))\n",
    "            save_model(best_model_finalized, save_folder + \"/best_model_finalized\")\n",
    "            save_model(best_model_finalized, save_folder + \"/best_blend_finalized\")\n",
    "    else:\n",
    "        save_model(best_model_finalized, \"best_model_finalized\")\n",
    "        save_model(best_model_finalized, \"best_blend_finalized\")\n",
    "        get_logs().to_csv(\"{}_{}_log.csv\".format(coin, return_period))\n",
    "  \n",
    "    return best_models, best_model_finalized, best_blend_finalized, get_logs()\n",
    "\n",
    "\n",
    "\n",
    "#=========================================================\n",
    "#====================CROSS-CORRELATION====================\n",
    "#=========================================================\n",
    "\n",
    "def cross_corr(crypto_wide, coin1, coin2, lag):\n",
    "    if lag > 0:\n",
    "        return(np.corrcoef(crypto_wide.dropna(subset=[coin1, coin2])[coin1].iloc[lag:], crypto_wide.dropna(subset=[coin1, coin2])[coin2].shift(lag).dropna())[0, 1])\n",
    "    elif lag < 0:\n",
    "        return(np.corrcoef(crypto_wide.dropna(subset=[coin1, coin2])[coin1].iloc[:lag], crypto_wide.dropna(subset=[coin1, coin2])[coin2].shift(lag).dropna())[0, 1])\n",
    "    elif lag == 0:\n",
    "        return(np.corrcoef(crypto_wide.dropna(subset=[coin1, coin2])[coin1], crypto_wide.dropna(subset=[coin1, coin2])[coin2])[0, 1])\n",
    "\n",
    "\n",
    "def cross_corr_range(crypto_wide, coin1, coin2, lag_range, plot=True):\n",
    "    result = pd.Series(lag_range).apply(lambda x: cross_corr(crypto_wide, coin1, coin2, int(round(x))))\n",
    "    result.index = lag_range\n",
    "    if plot:\n",
    "        plt.plot(result)\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_cross_corr_matrix(crypto_wide, price_cols, num_est = 10):\n",
    "    cross_corr_mat = pd.DataFrame([], columns=price_cols, index=price_cols)\n",
    "    print(\"Getting cross-correlations\")\n",
    "    print(\"\\t\", end=\"\")\n",
    "    for pair in tqdm(list(combinations(price_cols, 2))):\n",
    "        cross_corr_values = cross_corr_range(crypto_wide, pair[0], pair[1], np.linspace(-2880, 2880, num=num_est), plot=False)\n",
    "        greatest_result = cross_corr_values[cross_corr_values == cross_corr_values.max()]\n",
    "        greatest_lag = greatest_result.index[0]\n",
    "        greatest_corr = greatest_result.values[0]\n",
    "        if greatest_lag < 0:\n",
    "            cross_corr_mat.loc[pair[0], pair[1]] = greatest_corr\n",
    "        elif greatest_lag > 0:\n",
    "            cross_corr_mat.loc[pair[1], pair[0]] = greatest_corr\n",
    "        else:\n",
    "            pass\n",
    "    return cross_corr_mat.astype(float)\n",
    "\n",
    "\n",
    "def get_split_dates(crypto_wide, num_weeks_train, num_weeks_test, num_weeks_holdout):\n",
    "    # Defining our periods of interest\n",
    "    holdout_start = crypto_wide.index.max() - pd.Timedelta(weeks=num_weeks_holdout)\n",
    "    test_start = holdout_start - pd.Timedelta(weeks=num_weeks_test)\n",
    "    train_start = test_start - pd.Timedelta(weeks=num_weeks_train)\n",
    "    print(\"Number of training samples: {}\".format(((crypto_wide.index < test_start) & (crypto_wide.index >= train_start)).sum()))\n",
    "    print(\"Number of test samples: {}\".format(((crypto_wide.index >= test_start) & (crypto_wide.index < holdout_start)).sum()))\n",
    "    print(\"Number of holdout samples: {}\".format(((crypto_wide.index >= holdout_start)).sum()))\n",
    "    return train_start, test_start, holdout_start\n",
    "\n",
    "\n",
    "def prepare_data(crypto_wide, coin_of_interest, columns_of_interest, train_start, test_start, holdout_start, cluster_num=None, \n",
    "                  cross_correlation_matrix=None, cross_corr_thresh=0.50, outlier_column=False):\n",
    "    all_coins = [coin_of_interest]\n",
    "\n",
    "    # Yielding our testing/training/holdout dataframes\n",
    "    crypto_train = crypto_wide.loc[train_start:test_start]\n",
    "    crypto_test = crypto_wide.loc[test_start:holdout_start]\n",
    "    crypto_holdout = crypto_wide.loc[holdout_start:]\n",
    "    crypto_test_holdout = pd.concat([crypto_test, crypto_holdout])\n",
    "\n",
    "    # Getting clusters for each coin return\n",
    "    print(\"Fitting clusters on training data\")\n",
    "    print(\"\\t\", end=\"\")\n",
    "    cluster_cols = [col for col in crypto_train if \"{}_pct_change\".format(coin_of_interest) in col]\n",
    "    crypto_return_clusters_train, cluster_model_dict, order_dict = clusterize(crypto_wide.loc[train_start:test_start, cluster_cols], cluster_num=cluster_num)\n",
    "#     crypto_return_clusters_train = crypto_return_clusters_train.loc[train_start:]\n",
    "    crypto_return_clusters_test_holdout = crypto_test_holdout[cluster_cols].copy()\n",
    "\n",
    "    print(\"Predicting clusters on test and holdout data\")\n",
    "    print(\"\\t\", end=\"\")\n",
    "    for col in tqdm(cluster_cols):\n",
    "        kmean_labels = cluster_model_dict[col].predict(crypto_return_clusters_test_holdout[col].values.reshape(-1, 1))\n",
    "        crypto_return_clusters_test_holdout[col] = list(map(lambda x: order_dict[col][x], kmean_labels))\n",
    "    \n",
    "    crypto_return_clusters_train = crypto_return_clusters_train.astype(int).astype(str)\n",
    "    crypto_return_clusters_test_holdout = crypto_return_clusters_test_holdout.astype(int).astype(str)\n",
    "    crypto_train = crypto_train.join(crypto_return_clusters_train, lsuffix=\"\", rsuffix=\"_clust\")\n",
    "    crypto_test_holdout = crypto_test_holdout.join(crypto_return_clusters_test_holdout, lsuffix=\"\", rsuffix=\"_clust\")\n",
    "    columns_of_interest += [col for col in crypto_train if \"clust\" in col]\n",
    "  \n",
    "    # Adding coin data for potentially causal coins\n",
    "    if cross_correlation_matrix is not None:\n",
    "        print(\"Getting additional potentially influential coins\")\n",
    "        print(\"\\t\", end=\"\")\n",
    "        additional_coins = (cross_correlation_matrix.abs() > cross_corr_thresh).query(coin_of_interest).index.tolist()\n",
    "        all_coins += additional_coins\n",
    "        additional_cols = []\n",
    "        for coin in tqdm(additional_coins):\n",
    "            additional_cols += [col for col in crypto_train if coin in col]\n",
    "        columns_of_interest += additional_cols\n",
    "        columns_of_interest = list(set(columns_of_interest))\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "    # Getting coin-based outliers\n",
    "    if outlier_column:\n",
    "        print(\"Getting outliers\")\n",
    "        print(\"\\t\", end=\"\")\n",
    "        for coin in tqdm(all_coins):\n",
    "            # Getting coin-related columns to create outlier computation\n",
    "            coin_cols = [col for col in crypto_wide if coin in col]\n",
    "\n",
    "            # training the model\n",
    "            clf = IsolationForest(max_samples=100, random_state=1)\n",
    "            clf.fit(crypto_train[coin_cols].dropna().values)\n",
    "            train_outliers = clf.predict(crypto_train[coin_cols].dropna().values)\n",
    "            outlier_col = \"{}_OUTLIER\".format(coin)\n",
    "            columns_of_interest += [outlier_col]\n",
    "            crypto_train[outlier_col] = np.nan\n",
    "            crypto_train.loc[~crypto_train[coin_cols].isna().any(axis=1), outlier_col] = train_outliers.astype(int).astype(str)\n",
    "\n",
    "            # predicting model on test/holdout partitions\n",
    "            test_outliers = clf.predict(crypto_test_holdout[coin_cols].dropna().values)\n",
    "            crypto_test_holdout[outlier_col] = np.nan\n",
    "            crypto_test_holdout.loc[~crypto_test_holdout[coin_cols].isna().any(axis=1), outlier_col] = test_outliers.astype(int).astype(str)\n",
    "  \n",
    "    # Selecting only our columns of interest\n",
    "    crypto_train = crypto_train[columns_of_interest]\n",
    "    crypto_test_holdout = crypto_test_holdout[columns_of_interest]\n",
    "    return crypto_train, crypto_test_holdout, crypto_return_clusters_train, crypto_return_clusters_test_holdout\n",
    "\n",
    "\n",
    "def add_target(coin_of_interest, crypto_train, crypto_test_holdout, return_period, test_start):\n",
    "    # Joining on clusters of our coin returns\n",
    "    target = \"{}_pct_change_{}\".format(coin_of_interest, return_period)\n",
    "    combined_df = pd.concat([crypto_train, crypto_test_holdout])\n",
    "    combined_cluster_df = pd.concat([crypto_return_clusters_train, crypto_return_clusters_test_holdout])\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
    "    combined_cluster_df = combined_cluster_df[~combined_cluster_df.index.duplicated(keep='first')]\n",
    "    combined_df[\"target\"] = combined_cluster_df[target].shift(-return_period)\n",
    "    crypto_train, crypto_test_holdout = combined_df.loc[:test_start], combined_df.loc[test_start:]\n",
    "    cluster_desc = crypto_train.groupby(crypto_return_clusters_train[target])[[target]].describe()\n",
    "    print(\"Unique values of target: {}\".format(crypto_train[\"target\"].nunique()))\n",
    "    return crypto_train, crypto_test_holdout, cluster_desc\n",
    "\n",
    "\n",
    "def split_df(crypto_train, crypto_test_holdout, holdout_start):\n",
    "    # Further partitioning our combined test/holdout dataframe\n",
    "    crypto_test, crypto_holdout = crypto_test_holdout.loc[:holdout_start].dropna(subset=[\"target\"]), crypto_test_holdout.loc[holdout_start:].dropna(subset=[\"target\"])\n",
    "\n",
    "    # Converting target to string for classification task\n",
    "    crypto_train[\"target\"] = crypto_train[\"target\"].astype(int).astype(str)\n",
    "    crypto_test[\"target\"] = crypto_test[\"target\"].astype(int).astype(str)\n",
    "    crypto_test[\"target\"] = crypto_test[\"target\"].astype(int).astype(str)\n",
    "    return crypto_train, crypto_test, crypto_holdout\n",
    "\n",
    "\n",
    "#=============================================================\n",
    "#====================BACKTESTING UTILITIES====================\n",
    "#=============================================================\n",
    "\n",
    "class TimeFrame():\n",
    "    def __init__(self, data=None, index_col=None):\n",
    "        \"\"\"Initializing data\"\"\"\n",
    "        if data is None:\n",
    "            self.data = pd.DataFrame()\n",
    "        else:\n",
    "            self.data = data\n",
    "            if index_col is None:\n",
    "                pass\n",
    "            else:\n",
    "                self.data.set_index(index_col)\n",
    "        if (self.data.index.dtype.__str__() in ['datetime64[ns]', 'datetime64']) or (self.data.empty):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Requires one-dimensional datetime index\")\n",
    "            raise ValueError(\"index not datetime error\")\n",
    "    \n",
    "    \n",
    "    def check_datetime_index(self):\n",
    "        \"\"\"Check and enforce the main data index to a single-dimensional datetime\"\"\"\n",
    "        if (self.data.index.dtype.__str__() in ['datetime64[ns]', 'datetime64']) or (self.data.empty):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Requires one-dimensional datetime index\")\n",
    "            raise ValueError(\"index not datetime error\")\n",
    "            \n",
    "            \n",
    "    def add_data(self, data, index_col=None, values=None, lsuffix=None, rsuffix=None):\n",
    "        \"\"\"Joining another data source onto main data\"\"\"\n",
    "        if not self.data.empty:\n",
    "            if index_col is None:\n",
    "                self.data = self.data.join(data, lsuffix=lsuffix, rsuffix=rsuffix)\n",
    "            else:\n",
    "                self.data = self.data.join(data.set_index(index_col), lsuffix=lsuffix, rsuffix=rsuffix)\n",
    "        else:\n",
    "            self.data = data\n",
    "            if index_col is None:\n",
    "                pass\n",
    "            else:\n",
    "                self.data.set_index(index_col)\n",
    "        self.check_datetime_index()\n",
    "    \n",
    "    \n",
    "    def load_data(self, fname, index_col=None):\n",
    "        \"\"\"Add data by loading a local csv\"\"\"\n",
    "        data = pd.read_csv(fname)\n",
    "        self.add_data(data, index_col=index_col)\n",
    "    \n",
    "    \n",
    "    def multiply(self, col1, col2, out=None):\n",
    "        \"\"\"Multiply two columns together\"\"\"\n",
    "        if out is None:\n",
    "            out = col1 + \"_times_\" + col2\n",
    "        self.data[out] = self.data[col1] * self.data[col2]\n",
    "        self.check_datetime_index()\n",
    "        \n",
    "    \n",
    "    def divide(self, col1, col2, out=None):\n",
    "        \"\"\"Dividing two columns\"\"\"\n",
    "        if out is None:\n",
    "            out = col1 + \"_by_\" + col2\n",
    "        self.data[out] = self.data[col1] / self.data[col2].replace(0, np.nan)\n",
    "        self.check_datetime_index()\n",
    "    \n",
    "    \n",
    "    def aggregate(self, index_col=None, agg_dict=None):\n",
    "        \"\"\"Aggregate data at a specific level\"\"\"\n",
    "        if index_col is None:\n",
    "            groupby_col = self.data.index\n",
    "        else:\n",
    "            groupby_col = self.data[index_col]\n",
    "        \n",
    "        numeric_cols = self.data.dtypes[self.data.dtypes == float].index.tolist()\n",
    "        if agg_dict is None:\n",
    "            self.data = self.data.groupby(groupby_col)[[numeric_cols]].sum()\n",
    "        else:\n",
    "            self.data = self.data.groupby(groupby_col).agg(agg_dict)\n",
    "        self.check_datetime_index()\n",
    "\n",
    "\n",
    "class StockStrategy(TimeFrame):\n",
    "    def __init__(self, data, price_col):\n",
    "        super().__init__(data)\n",
    "        self.price_col = price_col\n",
    "        \n",
    "        # Initializing with a null strategy\n",
    "        self.signals = pd.DataFrame(index=data.index).sort_index()\n",
    "        self.signals[\"signal\"] = 0.0   \n",
    "    \n",
    "    \n",
    "    #---------------------GENERATING STRATEGIES-------------------------\n",
    "    #-------------------------------------------------------------------\n",
    "    def generic_query_strategy(self, query, exit_query=None, plot=False):\n",
    "        \"\"\"Creating a strategy based on cross_col1 > cross_col2, hold\"\"\"\n",
    "        self.signals = pd.DataFrame(index=self.data.index).sort_index()\n",
    "        self.signals[\"signal\"] = 0.0\n",
    "        if exit_query is not None:\n",
    "            buy_ind = self.data.query(query).index\n",
    "            sell_ind = self.data.query(exit_query).index\n",
    "            buy_sell = pd.Series(self.data.index.isin(buy_ind), index=self.data.index).map({True:1, False:np.nan})\n",
    "            sell_times = pd.Series(self.data.index.isin(sell_ind), index=self.data.index).map({True:-1, False:np.nan}).dropna()\n",
    "            buy_sell = pd.concat([sell_times, buy_sell])\n",
    "            buy_sell = buy_sell[~buy_sell.index.duplicated(keep=\"first\")]\n",
    "            buy_sell = buy_sell.sort_index()\n",
    "\n",
    "            curr_signal = 0\n",
    "            signal_ind = []\n",
    "            for x in buy_sell.iteritems():\n",
    "                if x[1] == 1:\n",
    "                    curr_signal = 1\n",
    "                elif x[1] == -1:\n",
    "                    curr_signal = 0\n",
    "                else:\n",
    "                    pass\n",
    "                signal_ind.append(curr_signal)\n",
    "\n",
    "            hold_signal = pd.Series(signal_ind, index=buy_sell.index)\n",
    "            self.signals.loc[hold_signal.astype(bool)] = 1\n",
    "        else:\n",
    "            self.signals.loc[self.data.query(query).index] = 1\n",
    "        \n",
    "        self.signals[\"positions\"] = self.signals[\"signal\"].diff()\n",
    "        self.strategy_query = query\n",
    "        \n",
    "        if plot:\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_size_inches(18.5, 10.5)\n",
    "            self.data[self.price_col].sort_index().reset_index().astype({\"Time\":str}).set_index(\"Time\").plot(ax=ax)\n",
    "            ax2 = ax.twinx()\n",
    "            self.signals[\"signal\"].sort_index().reset_index().astype({\"Time\":str}).set_index(\"Time\").plot(ax=ax2, color=\"pink\")    \n",
    "    \n",
    "    \n",
    "    #---------------------BACKTESTING STRATEGIES-------------------------\n",
    "    #--------------------------------------------------------------------\n",
    "    def backtest(self, initial_capital=10000, shares_per_position=100):\n",
    "        # Create a DataFrame `positions`\n",
    "        self.positions = pd.DataFrame(index=self.signals.index).fillna(0.0)\n",
    "\n",
    "        # Buy a 100 shares\n",
    "        self.positions['Stock'] = shares_per_position*self.signals['signal']   \n",
    "\n",
    "        # Initialize the portfolio with value owned   \n",
    "        portfolio = self.positions.multiply(self.data[self.price_col], axis=0)\n",
    "\n",
    "        # Store the difference in shares owned \n",
    "        pos_diff = self.positions.sort_index().diff()\n",
    "\n",
    "        # Add `holdings` to portfolio\n",
    "        portfolio['holdings'] = (self.positions.multiply(self.data[self.price_col], axis=0)).sum(axis=1)\n",
    "\n",
    "        # Add `cash` to portfolio\n",
    "        portfolio['cash'] = initial_capital - (pos_diff.multiply(self.data[self.price_col], axis=0)).sum(axis=1).cumsum()   \n",
    "\n",
    "        # Add `total` to portfolio\n",
    "        portfolio['total'] = portfolio['cash'] + portfolio['holdings']\n",
    "\n",
    "        # Add `returns` to portfolio\n",
    "        portfolio['returns'] = portfolio['total'].pct_change()\n",
    "        portfolio = portfolio.join(self.data[[self.price_col]])\n",
    "        \n",
    "        self.portfolio = portfolio\n",
    "    \n",
    "    def plot_backtest(self, date):\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        ax1 = fig.add_subplot(111, ylabel='Portfolio value in $')\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        if date == \"all\":\n",
    "            day_portfolio = self.portfolio.sort_index().reset_index().astype({\"Time\":str}).set_index(\"Time\")\n",
    "            day_signals = self.signals.sort_index().reset_index().astype({\"Time\":str}).set_index(\"Time\")\n",
    "        else:\n",
    "            day_portfolio = self.portfolio.loc[self.portfolio.index.date == pd.Timestamp(date)]\n",
    "            day_signals = self.signals.loc[self.signals.index.date == pd.Timestamp(date)]\n",
    "\n",
    "        # Plot the equity curve in dollars\n",
    "        day_portfolio['total'].plot(ax=ax1, lw=2.0)\n",
    "        day_portfolio[self.price_col].plot(ax=ax2, color=\"orange\")\n",
    "\n",
    "        # Plot the \"buy\" trades against the equity curve\n",
    "        if date != \"all\":\n",
    "            ax1.plot(day_portfolio.loc[day_signals.positions == 1.0].index, \n",
    "                     day_portfolio.total[day_signals.positions == 1.0],\n",
    "                     '^', markersize=10, color='m')\n",
    "\n",
    "            # Plot the \"sell\" trades against the equity curve\n",
    "            ax1.plot(day_portfolio.loc[day_signals.positions == -1.0].index, \n",
    "                     day_portfolio.total[day_signals.positions == -1.0],\n",
    "                     'v', markersize=10, color='k')\n",
    "        plt.title(\"Portfolio Value for {}\".format(date))\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "    #----------------------EVALUATING STRATEGIES-------------------------\n",
    "    #--------------------------------------------------------------------\n",
    "    def evaluate_strategy(self, metric):\n",
    "        if metric == \"sharpe\":\n",
    "            # Isolate the returns of your strategy\n",
    "            returns = self.portfolio['returns']\n",
    "\n",
    "            # annualized Sharpe ratio\n",
    "            sharpe_ratio = np.sqrt(len(returns)) * (returns.mean() / returns.std())\n",
    "\n",
    "            # Print the Sharpe ratio\n",
    "            return sharpe_ratio\n",
    "        elif metric == \"max drawdown\":\n",
    "            # Define a trailing 252 trading day window\n",
    "            window = 60\n",
    "            \n",
    "            # Calculate the max drawdown in the past window days for each day\n",
    "            rolling_max = self.portfolio[\"total\"].sort_index().rolling(window, min_periods=1).max()\n",
    "            hourly_drawdown = self.portfolio[\"total\"]/rolling_max - 1.0\n",
    "\n",
    "            # Calculate the minimum (negative) daily drawdown\n",
    "            max_hourly_drawdown = hourly_drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "            # Plot the results\n",
    "            hourly_drawdown.plot()\n",
    "            max_hourly_drawdown.plot()\n",
    "\n",
    "            # Show the plot\n",
    "            plt.show()\n",
    "        elif metric == \"cdgr\":\n",
    "            # Get the number of days in `aapl`\n",
    "            mins = pd.Timedelta(self.portfolio.index[-1] - self.portfolio.index[0]).seconds/60\n",
    "\n",
    "            # Calculate the CAGR\n",
    "            cdgr = ((((self.portfolio[self.price_col][-1]) / self.portfolio[self.price_col][1])) ** (3600/mins)) - 1\n",
    "\n",
    "            # Print CAGR\n",
    "            return cdgr\n",
    "    \n",
    "    #------------------------SAVING STRATEGIES---------------------------\n",
    "    #--------------------------------------------------------------------\n",
    "    def save_strategy(self, filename=\"strategy.stgy\"):\n",
    "        f = open(filename, \"w+\")\n",
    "        f.write(self.strategy_query)\n",
    "        f.close()\n",
    "        \n",
    "    \n",
    "    \n",
    "    #---------------------------PLOTTING------------------------------\n",
    "    #-----------------------------------------------------------------\n",
    "    def plot_prices(self, date):\n",
    "        fig = plt.figure()\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        ax1 = fig.add_subplot(111, ylabel='Portfolio value in $')\n",
    "        date_df = self.data.loc[self.data.index.date == pd.Timestamp(date)]\n",
    "        date_df[self.price_col].plot(ax=ax1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Expanded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: ········\n",
      "password: ········\n",
      "pct_change\n",
      "diff\n",
      "beta\n",
      "--------AVAXUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): AVAXUSDT_beta_avg    0.000131\n",
      "dtype: float64\n",
      "--------BATUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): BATUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------BTCUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): BTCUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------CHZUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): CHZUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------DOTUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): DOTUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------ENJUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): ENJUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------ETHUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): ETHUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------FTMUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): FTMUSDT_beta_avg    0.000131\n",
      "dtype: float64\n",
      "--------GALAUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): GALAUSDT_beta_avg    0.734238\n",
      "dtype: float64\n",
      "--------LINKUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): LINKUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------LRCUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): LRCUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------MANAUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): MANAUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "--------MATICUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): MATICUSDT_beta_avg    0.000131\n",
      "dtype: float64\n",
      "--------SOLUSDT\n",
      "--------tmp.shape:(458309, 1)\n",
      "--------tmp.isna().mean(): SOLUSDT_beta_avg    0.003273\n",
      "dtype: float64\n",
      "rsi_bb\n",
      "--------AVAXUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): AVAXUSDT_AVAXUSDT    0.003142\n",
      "AVAXUSDT_RSI         0.000031\n",
      "AVAXUSDT_LOW_2       0.000249\n",
      "AVAXUSDT_MID         0.000249\n",
      "AVAXUSDT_UP_2        0.000249\n",
      "AVAXUSDT_LOW_1       0.000249\n",
      "AVAXUSDT_UP_1        0.000249\n",
      "AVAXUSDT_BBP_2       0.003391\n",
      "AVAXUSDT_BBP_1       0.003391\n",
      "dtype: float64\n",
      "--------BATUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): BATUSDT_BATUSDT    0.003144\n",
      "BATUSDT_RSI        0.003173\n",
      "BATUSDT_LOW_2      0.003391\n",
      "BATUSDT_MID        0.003391\n",
      "BATUSDT_UP_2       0.003391\n",
      "BATUSDT_LOW_1      0.003391\n",
      "BATUSDT_UP_1       0.003391\n",
      "BATUSDT_BBP_2      0.003393\n",
      "BATUSDT_BBP_1      0.003393\n",
      "dtype: float64\n",
      "--------BTCUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): BTCUSDT_BTCUSDT    0.003144\n",
      "BTCUSDT_RSI        0.003173\n",
      "BTCUSDT_LOW_2      0.003391\n",
      "BTCUSDT_MID        0.003391\n",
      "BTCUSDT_UP_2       0.003391\n",
      "BTCUSDT_LOW_1      0.003391\n",
      "BTCUSDT_UP_1       0.003391\n",
      "BTCUSDT_BBP_2      0.003393\n",
      "BTCUSDT_BBP_1      0.003393\n",
      "dtype: float64\n",
      "--------CHZUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): CHZUSDT_CHZUSDT    0.003144\n",
      "CHZUSDT_RSI        0.003173\n",
      "CHZUSDT_LOW_2      0.003391\n",
      "CHZUSDT_MID        0.003391\n",
      "CHZUSDT_UP_2       0.003391\n",
      "CHZUSDT_LOW_1      0.003391\n",
      "CHZUSDT_UP_1       0.003391\n",
      "CHZUSDT_BBP_2      0.003393\n",
      "CHZUSDT_BBP_1      0.003393\n",
      "dtype: float64\n",
      "--------DOTUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): DOTUSDT_DOTUSDT    0.003144\n",
      "DOTUSDT_RSI        0.003173\n",
      "DOTUSDT_LOW_2      0.003391\n",
      "DOTUSDT_MID        0.003391\n",
      "DOTUSDT_UP_2       0.003391\n",
      "DOTUSDT_LOW_1      0.003391\n",
      "DOTUSDT_UP_1       0.003391\n",
      "DOTUSDT_BBP_2      0.003393\n",
      "DOTUSDT_BBP_1      0.003393\n",
      "dtype: float64\n",
      "--------ENJUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): ENJUSDT_ENJUSDT    0.003144\n",
      "ENJUSDT_RSI        0.003173\n",
      "ENJUSDT_LOW_2      0.003391\n",
      "ENJUSDT_MID        0.003391\n",
      "ENJUSDT_UP_2       0.003391\n",
      "ENJUSDT_LOW_1      0.003391\n",
      "ENJUSDT_UP_1       0.003391\n",
      "ENJUSDT_BBP_2      0.003393\n",
      "ENJUSDT_BBP_1      0.003393\n",
      "dtype: float64\n",
      "--------ETHUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): ETHUSDT_ETHUSDT    0.003144\n",
      "ETHUSDT_RSI        0.003173\n",
      "ETHUSDT_LOW_2      0.003391\n",
      "ETHUSDT_MID        0.003391\n",
      "ETHUSDT_UP_2       0.003391\n",
      "ETHUSDT_LOW_1      0.003391\n",
      "ETHUSDT_UP_1       0.003391\n",
      "ETHUSDT_BBP_2      0.003393\n",
      "ETHUSDT_BBP_1      0.003393\n",
      "dtype: float64\n",
      "--------FTMUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): FTMUSDT_FTMUSDT    0.003142\n",
      "FTMUSDT_RSI        0.000031\n",
      "FTMUSDT_LOW_2      0.000249\n",
      "FTMUSDT_MID        0.000249\n",
      "FTMUSDT_UP_2       0.000249\n",
      "FTMUSDT_LOW_1      0.000249\n",
      "FTMUSDT_UP_1       0.000249\n",
      "FTMUSDT_BBP_2      0.003391\n",
      "FTMUSDT_BBP_1      0.003391\n",
      "dtype: float64\n",
      "--------GALAUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): GALAUSDT_GALAUSDT    0.734107\n",
      "GALAUSDT_RSI         0.734138\n",
      "GALAUSDT_LOW_2       0.734356\n",
      "GALAUSDT_MID         0.734356\n",
      "GALAUSDT_UP_2        0.734356\n",
      "GALAUSDT_LOW_1       0.734356\n",
      "GALAUSDT_UP_1        0.734356\n",
      "GALAUSDT_BBP_2       0.734356\n",
      "GALAUSDT_BBP_1       0.734356\n",
      "dtype: float64\n",
      "--------LINKUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): LINKUSDT_LINKUSDT    0.003144\n",
      "LINKUSDT_RSI         0.003173\n",
      "LINKUSDT_LOW_2       0.003391\n",
      "LINKUSDT_MID         0.003391\n",
      "LINKUSDT_UP_2        0.003391\n",
      "LINKUSDT_LOW_1       0.003391\n",
      "LINKUSDT_UP_1        0.003391\n",
      "LINKUSDT_BBP_2       0.003393\n",
      "LINKUSDT_BBP_1       0.003393\n",
      "dtype: float64\n",
      "--------LRCUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): LRCUSDT_LRCUSDT    0.003142\n",
      "LRCUSDT_RSI        0.003173\n",
      "LRCUSDT_LOW_2      0.003391\n",
      "LRCUSDT_MID        0.003391\n",
      "LRCUSDT_UP_2       0.003391\n",
      "LRCUSDT_LOW_1      0.003391\n",
      "LRCUSDT_UP_1       0.003391\n",
      "LRCUSDT_BBP_2      0.003391\n",
      "LRCUSDT_BBP_1      0.003391\n",
      "dtype: float64\n",
      "--------MANAUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): MANAUSDT_MANAUSDT    0.003144\n",
      "MANAUSDT_RSI         0.003173\n",
      "MANAUSDT_LOW_2       0.003391\n",
      "MANAUSDT_MID         0.003391\n",
      "MANAUSDT_UP_2        0.003391\n",
      "MANAUSDT_LOW_1       0.003391\n",
      "MANAUSDT_UP_1        0.003391\n",
      "MANAUSDT_BBP_2       0.003393\n",
      "MANAUSDT_BBP_1       0.003393\n",
      "dtype: float64\n",
      "--------MATICUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): MATICUSDT_MATICUSDT    0.003142\n",
      "MATICUSDT_RSI          0.000031\n",
      "MATICUSDT_LOW_2        0.000249\n",
      "MATICUSDT_MID          0.000249\n",
      "MATICUSDT_UP_2         0.000249\n",
      "MATICUSDT_LOW_1        0.000249\n",
      "MATICUSDT_UP_1         0.000249\n",
      "MATICUSDT_BBP_2        0.003391\n",
      "MATICUSDT_BBP_1        0.003391\n",
      "dtype: float64\n",
      "--------SOLUSDT\n",
      "--------tmp.shape:(458309, 9)\n",
      "--------tmp.isna().mean(): SOLUSDT_SOLUSDT    0.003144\n",
      "SOLUSDT_RSI        0.003173\n",
      "SOLUSDT_LOW_2      0.003391\n",
      "SOLUSDT_MID        0.003391\n",
      "SOLUSDT_UP_2       0.003391\n",
      "SOLUSDT_LOW_1      0.003391\n",
      "SOLUSDT_UP_1       0.003391\n",
      "SOLUSDT_BBP_2      0.003393\n",
      "SOLUSDT_BBP_1      0.003393\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "username = getpass(\"username: \")\n",
    "password = getpass(\"password: \")\n",
    "\n",
    "# Reading in data from snowflake\n",
    "conn  = snowflake.connector.connect(user=username,\n",
    "                                   password=password,\n",
    "                                   account=\"kga72450.us-east-1\")\n",
    "\n",
    "conn.cursor().execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "conn.cursor().execute(\"USE DATABASE CRYPTO\")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql = \"select * from TOP_CRYPTO_YTD\"\n",
    "cur.execute(sql)\n",
    "crypto_df = cur.fetch_pandas_all()\n",
    "\n",
    "# Converting types\n",
    "crypto_df = crypto_df.astype({col:float for col in crypto_df.columns if col not in [\"NA\", \"COIN\"]})\n",
    "\n",
    "# Converting timestamps to date-time\n",
    "convert_time = lambda x: datetime.fromtimestamp(x/1000)# Convert from millisecond to second\n",
    "crypto_df[\"OPEN_TIME\"] = crypto_df[\"OPEN_TIME\"].apply(convert_time)\n",
    "crypto_df[\"CLOSE TIME\"] = crypto_df[\"CLOSE TIME\"].apply(convert_time)\n",
    "\n",
    "# Pivoting columns to wide\n",
    "crypto_open = crypto_df.pivot_table(columns=[\"COIN\"], index=[\"OPEN_TIME\"], values=[\"OPEN\"])[\"OPEN\"]\n",
    "crypto_volume = crypto_df.pivot_table(columns=[\"COIN\"], index=[\"OPEN_TIME\"], values=[\"VOLUME\"])[\"VOLUME\"]\n",
    "crypto_num_trade = crypto_df.pivot_table(columns=[\"COIN\"], index=[\"OPEN_TIME\"], values=[\"NUMBER_OF_TRADES\"])[\"NUMBER_OF_TRADES\"]\n",
    "\n",
    "crypto_seed_df = crypto_open.join(crypto_volume.join(crypto_num_trade, lsuffix=\"_VOLUME\", rsuffix=\"_NUM_TRADES\"))# Joining everything together\n",
    "volume_cols = [col for col in crypto_seed_df if \"VOLUME\" in col]\n",
    "num_trades_cols = [col for col in crypto_seed_df if \"NUM_TRADES\" in col]\n",
    "price_cols = [col for col in crypto_seed_df if col.endswith(\"USDT\")]\n",
    "all_cols = volume_cols + num_trades_cols + price_cols\n",
    "\n",
    "col_dict = {\"diff\":(num_trades_cols + volume_cols), \"pct_change\":price_cols, \"beta\":price_cols, \\\n",
    "            \"direct_mov\":price_cols, \"rsi_bb\":price_cols, \"sp_coin\":price_cols}\n",
    "\n",
    "# Getting data with all the indices for analysis\n",
    "crypto_wide = get_all_indices(crypto_seed_df, rolling_periods=[1, 5, 10, 60, 180], groupby_col=None, \\\n",
    "                              metrics=[\"pct_change\", \"diff\", \"beta\", \"rsi_bb\"], \\\n",
    "                              col_dict=col_dict)\n",
    "crypto_wide = crypto_wide.reset_index().rename(columns={\"OPEN_TIME\":\"Time\"}).set_index(\"Time\")\n",
    "del crypto_seed_df, crypto_num_trade, crypto_volume, crypto_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_522a0_row5_col0,#T_522a0_row5_col1,#T_522a0_row5_col2,#T_522a0_row5_col3,#T_522a0_row5_col4,#T_522a0_row5_col5,#T_522a0_row5_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_522a0_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_522a0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_522a0_row0_col0\" class=\"data row0 col0\" >0.4417</td>\n",
       "                        <td id=\"T_522a0_row0_col1\" class=\"data row0 col1\" >0.6263</td>\n",
       "                        <td id=\"T_522a0_row0_col2\" class=\"data row0 col2\" >0.3226</td>\n",
       "                        <td id=\"T_522a0_row0_col3\" class=\"data row0 col3\" >0.4087</td>\n",
       "                        <td id=\"T_522a0_row0_col4\" class=\"data row0 col4\" >0.3654</td>\n",
       "                        <td id=\"T_522a0_row0_col5\" class=\"data row0 col5\" >0.0850</td>\n",
       "                        <td id=\"T_522a0_row0_col6\" class=\"data row0 col6\" >0.1065</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_522a0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_522a0_row1_col0\" class=\"data row1 col0\" >0.5217</td>\n",
       "                        <td id=\"T_522a0_row1_col1\" class=\"data row1 col1\" >0.5796</td>\n",
       "                        <td id=\"T_522a0_row1_col2\" class=\"data row1 col2\" >0.2996</td>\n",
       "                        <td id=\"T_522a0_row1_col3\" class=\"data row1 col3\" >0.4839</td>\n",
       "                        <td id=\"T_522a0_row1_col4\" class=\"data row1 col4\" >0.4356</td>\n",
       "                        <td id=\"T_522a0_row1_col5\" class=\"data row1 col5\" >0.0429</td>\n",
       "                        <td id=\"T_522a0_row1_col6\" class=\"data row1 col6\" >0.0600</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_522a0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_522a0_row2_col0\" class=\"data row2 col0\" >0.5100</td>\n",
       "                        <td id=\"T_522a0_row2_col1\" class=\"data row2 col1\" >0.5889</td>\n",
       "                        <td id=\"T_522a0_row2_col2\" class=\"data row2 col2\" >0.3044</td>\n",
       "                        <td id=\"T_522a0_row2_col3\" class=\"data row2 col3\" >0.4970</td>\n",
       "                        <td id=\"T_522a0_row2_col4\" class=\"data row2 col4\" >0.4335</td>\n",
       "                        <td id=\"T_522a0_row2_col5\" class=\"data row2 col5\" >0.0613</td>\n",
       "                        <td id=\"T_522a0_row2_col6\" class=\"data row2 col6\" >0.0818</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_522a0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_522a0_row3_col0\" class=\"data row3 col0\" >0.5418</td>\n",
       "                        <td id=\"T_522a0_row3_col1\" class=\"data row3 col1\" >0.5600</td>\n",
       "                        <td id=\"T_522a0_row3_col2\" class=\"data row3 col2\" >0.3030</td>\n",
       "                        <td id=\"T_522a0_row3_col3\" class=\"data row3 col3\" >0.5199</td>\n",
       "                        <td id=\"T_522a0_row3_col4\" class=\"data row3 col4\" >0.5059</td>\n",
       "                        <td id=\"T_522a0_row3_col5\" class=\"data row3 col5\" >0.0631</td>\n",
       "                        <td id=\"T_522a0_row3_col6\" class=\"data row3 col6\" >0.0695</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_522a0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_522a0_row4_col0\" class=\"data row4 col0\" >0.5598</td>\n",
       "                        <td id=\"T_522a0_row4_col1\" class=\"data row4 col1\" >0.5784</td>\n",
       "                        <td id=\"T_522a0_row4_col2\" class=\"data row4 col2\" >0.3061</td>\n",
       "                        <td id=\"T_522a0_row4_col3\" class=\"data row4 col3\" >0.5354</td>\n",
       "                        <td id=\"T_522a0_row4_col4\" class=\"data row4 col4\" >0.5333</td>\n",
       "                        <td id=\"T_522a0_row4_col5\" class=\"data row4 col5\" >0.0833</td>\n",
       "                        <td id=\"T_522a0_row4_col6\" class=\"data row4 col6\" >0.0882</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_522a0_level0_row5\" class=\"row_heading level0 row5\" >Mean</th>\n",
       "                        <td id=\"T_522a0_row5_col0\" class=\"data row5 col0\" >0.5150</td>\n",
       "                        <td id=\"T_522a0_row5_col1\" class=\"data row5 col1\" >0.5867</td>\n",
       "                        <td id=\"T_522a0_row5_col2\" class=\"data row5 col2\" >0.3072</td>\n",
       "                        <td id=\"T_522a0_row5_col3\" class=\"data row5 col3\" >0.4890</td>\n",
       "                        <td id=\"T_522a0_row5_col4\" class=\"data row5 col4\" >0.4547</td>\n",
       "                        <td id=\"T_522a0_row5_col5\" class=\"data row5 col5\" >0.0671</td>\n",
       "                        <td id=\"T_522a0_row5_col6\" class=\"data row5 col6\" >0.0812</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_522a0_level0_row6\" class=\"row_heading level0 row6\" >SD</th>\n",
       "                        <td id=\"T_522a0_row6_col0\" class=\"data row6 col0\" >0.0404</td>\n",
       "                        <td id=\"T_522a0_row6_col1\" class=\"data row6 col1\" >0.0219</td>\n",
       "                        <td id=\"T_522a0_row6_col2\" class=\"data row6 col2\" >0.0080</td>\n",
       "                        <td id=\"T_522a0_row6_col3\" class=\"data row6 col3\" >0.0440</td>\n",
       "                        <td id=\"T_522a0_row6_col4\" class=\"data row6 col4\" >0.0593</td>\n",
       "                        <td id=\"T_522a0_row6_col5\" class=\"data row6 col5\" >0.0156</td>\n",
       "                        <td id=\"T_522a0_row6_col6\" class=\"data row6 col6\" >0.0159</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1df3eea6340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n",
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "num_weeks_train, num_weeks_test, num_weeks_holdout = 4, 1, 1\n",
    "train_start, test_start, holdout_start = get_split_dates(crypto_wide, num_weeks_train, num_weeks_test, num_weeks_holdout)\n",
    "cross_corr_mat = get_cross_corr_matrix(crypto_wide.loc[train_start:test_start], price_cols, num_est=10)\n",
    "\n",
    "for coin_of_interest in set(price_cols) - {\"BATUSDT\", \"AVAXUSDT\", \"FTMUSDT\", \"MANAUSDT\", \"BTCUSDT\", \"DOTUSDT\", \"LRCUSDT\", \"GALAUSDT\", \"ETHUSDT\", \"ENJUSDT\", \"LINKUSDT\", \"SOLUSDT\", \"MATICUSDT\", \"CHZUSDT\"}:\n",
    "    crypto_train, crypto_test_holdout, crypto_return_clusters_train, crypto_return_clusters_test_holdout = prepare_data(crypto_wide, coin_of_interest, [col for col in crypto_wide if (\"BTCUSDT\" in col) or (\"ETHUSDT\" in col) or (coin_of_interest in col)], train_start=train_start, test_start=test_start, holdout_start=holdout_start, cluster_num=None, \n",
    "                      cross_correlation_matrix=cross_corr_mat, cross_corr_thresh=0.50, outlier_column=True)\n",
    "    return_periods = [5]\n",
    "#     return_periods = [1, 5, 10, 30, 60, 180]\n",
    "    test_args = {\"fold_strategy\":\"timeseries\", \"fold\":5}\n",
    "\n",
    "    for return_period in return_periods:\n",
    "        crypto_train_tmp, crypto_test_holdout_tmp, cluster_desc = add_target(coin_of_interest, crypto_train, crypto_test_holdout, return_period, test_start)\n",
    "        crypto_train_tmp, crypto_test_tmp, crypto_holdout_tmp = split_df(crypto_train_tmp, crypto_test_holdout_tmp, holdout_start)\n",
    "        best_models, best_model_finalized, best_blend_finalized, logs = pycaret_automl(crypto_train_tmp, crypto_test_tmp, coin_of_interest, return_period, plot=False, download=False, save_folder=\"./crypto_models/{}/{}\".format(coin_of_interest, return_period),\n",
    "                                                                                      pycaret_args=test_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
