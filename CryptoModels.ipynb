{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn\n",
      "  Downloading hmmlearn-0.2.6-cp38-cp38-win_amd64.whl (118 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from hmmlearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.10 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from hmmlearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from hmmlearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jiangian\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python[pandas] in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: pyOpenSSL<21.0.0,>=16.2.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (19.1.0)\n",
      "Requirement already satisfied: cryptography<4.0.0,>=3.1.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (3.1.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.0.1)\n",
      "Requirement already satisfied: pycryptodomex!=3.5.0,<4.0.0,>=3.2 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (3.10.1)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.14.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages (from snowflake-connector-python[pandas]) (2021.1)\n",
      "Requirement already satisfied: setuptools>34.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (50.3.1.post20201107)\n",
      "Requirement already satisfied: oscrypto<2.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.2.1)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.24.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (2020.12.5)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.4.0)\n",
      "Requirement already satisfied: pyarrow<5.1.0,>=5.0.0 in c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages (from snowflake-connector-python[pandas]) (5.0.0)\n",
      "Requirement already satisfied: pandas<1.4.0,>=1.0.0 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from snowflake-connector-python[pandas]) (1.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.20)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from cryptography<4.0.0,>=3.1.0->snowflake-connector-python[pandas]) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages (from pandas<1.4.0,>=1.0.0->snowflake-connector-python[pandas]) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from pandas<1.4.0,>=1.0.0->snowflake-connector-python[pandas]) (1.19.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jiangian\\anaconda3\\lib\\site-packages (from requests<3.0.0->snowflake-connector-python[pandas]) (1.25.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\jiangian\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nowflake-connector-python (c:\\users\\jiangian\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\jiangian\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user \"snowflake-connector-python[pandas]\" --upgrade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from kneed import DataGenerator, KneeLocator\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import rankdata\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# from stock.technical import get_all_indices\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "from pycaret.classification import *\n",
    "\n",
    "\n",
    "#===========================================================\n",
    "#====================FEATURE ENGINEERING====================\n",
    "#===========================================================\n",
    "\n",
    "def clusterize(df, cluster_num=2):\n",
    "    def find_cluster_num(data, max_clusters=10):\n",
    "        sse = {}\n",
    "        for k in range(1, max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=k)\n",
    "            kmeans.fit(data)\n",
    "            sse[k] = kmeans.inertia_\n",
    "        kn = KneeLocator(x=list(sse.keys()), \n",
    "                  y=list(sse.values()), \n",
    "                  curve='convex', \n",
    "                  direction='decreasing')\n",
    "        return kn.knee \n",
    "    df_out= df.copy()\n",
    "    clust_model_dict = {}\n",
    "    order_dict = {}\n",
    "    for col in tqdm(df.columns):\n",
    "        col_output = df[col]\n",
    "        col_output = col_output.replace([np.inf, -np.inf], np.nan)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            if cluster_num is None:\n",
    "                cluster_num_tmp = find_cluster_num(col_output.dropna().values.reshape(-1, 1))\n",
    "            else:\n",
    "                cluster_num_tmp = cluster_num\n",
    "            kmeans = KMeans(n_clusters=cluster_num_tmp).fit(col_output.dropna().values.reshape(-1, 1))\n",
    "            clust_model_dict[col] = kmeans\n",
    "            order_map = dict(zip(range(cluster_num_tmp), np.squeeze(rankdata(kmeans.cluster_centers_)).tolist()))\n",
    "            col_output[~col_output.isna()] = list(map(lambda x: order_map[x], kmeans.labels_))\n",
    "            order_dict[col] = order_map\n",
    "        df_out[col] = col_output\n",
    "    return df_out, clust_model_dict, order_dict\n",
    "\n",
    "\n",
    "def get_all_indices(data_final, rolling_periods, groupby_col=\"Day\", metrics=[\"std\", \"mean\", \"min\", \"max\", \"skew\", \"pct_change\"], col_dict=None):\n",
    "    if col_dict is None:\n",
    "        col_dict = {m:data_final.columns.tolist() for m in metrics}\n",
    "    data_out = data_final.copy()\n",
    "    for metric in metrics:\n",
    "        for period in rolling_periods:\n",
    "            if metric in [\"pct_change\", \"diff\"]:\n",
    "                if groupby_col is not None:\n",
    "                    tmp = getattr(data_final[col_dict[metric]].groupby(groupby_col), metric)(periods=period).droplevel(0)\n",
    "                else:\n",
    "                    tmp = getattr(data_final[col_dict[metric]], metric)(periods=period)\n",
    "            else:\n",
    "                if groupby_col is not None:\n",
    "                    tmp = getattr(data_final[col_dict[metric]].groupby(groupby_col).rolling(period, min_periods=1), metric)().droplevel(0)\n",
    "                else:\n",
    "                    tmp = getattr(data_final[col_dict[metric]].rolling(period, min_periods=1), metric)()\n",
    "            data_out = data_out.join(tmp, lsuffix=\"\", rsuffix=\"_{}_{}\".format(metric, period))\n",
    "    return data_out\n",
    "\n",
    "\n",
    "#===============================================\n",
    "#====================PYCARET====================\n",
    "#===============================================\n",
    "\n",
    "def pycaret_automl(crypto_train, crypto_test, coin, return_period, \n",
    "                   plot=True, download=False, save_folder=\"/content/drive/MyDrive/crypto_models\",\n",
    "                  pycaret_args=None):\n",
    "    # Setting up environment\n",
    "    # CRITICAL: Out-of-time validation scheme\n",
    "    print(\"'target' in crypto_train: {}\".format(\"target\" in crypto_train))\n",
    "    print(\"'target' in crypto_test: {}\".format(\"target\" in crypto_test))\n",
    "    \n",
    "    if pycaret_args is None:\n",
    "        exp_default = setup(data=crypto_train, test_data=crypto_test, target=\"target\", log_experiment=True, \n",
    "                            experiment_name=\"{}_{}\".format(coin, return_period), silent=True)\n",
    "    else:\n",
    "        exp_default = setup(data=crypto_train, test_data=crypto_test, target=\"target\", log_experiment=True, \n",
    "                    experiment_name=\"{}_{}\".format(coin, return_period), silent=True, **pycaret_args)\n",
    "    if save_folder is not None:\n",
    "        if os.path.exists(save_folder):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(save_folder)\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_models = compare_models(n_select=5, sort=\"AUC\", exclude=[\"gbc\"])\n",
    "    print(\"Completed in {} seconds\".format(time.time() - start_time))\n",
    "\n",
    "    for mdl in best_models:\n",
    "        mld_name = \"{}_{}_{}\".format(coin, mdl.__class__.__name__, return_period)\n",
    "        if save_folder is not None:\n",
    "            if save_folder.endswith(\"/\"):\n",
    "                save_model(mdl, save_folder + mld_name)\n",
    "            else:\n",
    "                save_model(mdl, save_folder + \"/\" + mld_name)\n",
    "        else:\n",
    "            save_model(mdl, mld_name)\n",
    "    \n",
    "    if download:\n",
    "        files.download(mld_name + \".pkl\")    \n",
    "  \n",
    "    best_model = best_models[0]# Selecting best model\n",
    "    best_models_blend = blend_models(best_models)\n",
    "\n",
    "    if plot:\n",
    "        print(\"========================================================================================================================================\")\n",
    "        print(\"===============================================================BEST MODEL===============================================================\")\n",
    "        print(\"========================================================================================================================================\")\n",
    "        plot_model(best_model)\n",
    "        plot_model(best_model, plot=\"confusion_matrix\")\n",
    "\n",
    "        print(\"========================================================================================================================================\")\n",
    "        print(\"===============================================================BEST BLEND===============================================================\")\n",
    "        print(\"========================================================================================================================================\")\n",
    "        plot_model(best_models_blend)\n",
    "        plot_model(best_models_blend, plot=\"confusion_matrix\")\n",
    "      \n",
    "    # Finalizing best model(s)\n",
    "    best_model_finalized = finalize_model(best_model)\n",
    "    best_blend_finalized = finalize_model(best_models_blend)\n",
    "  \n",
    "    if save_folder is not None:\n",
    "        if save_folder.endswith(\"/\"):\n",
    "            get_logs().to_csv(save_folder + \"{}_{}_log.csv\".format(coin, return_period))\n",
    "            save_model(best_model_finalized, save_folder + \"best_model_finalized\")\n",
    "            save_model(best_model_finalized, save_folder + \"best_blend_finalized\")\n",
    "        else:\n",
    "            get_logs().to_csv(save_folder + \"/{}_{}_log.csv\".format(coin, return_period))\n",
    "            save_model(best_model_finalized, save_folder + \"/best_model_finalized\")\n",
    "            save_model(best_model_finalized, save_folder + \"/best_blend_finalized\")\n",
    "    else:\n",
    "        save_model(best_model_finalized, \"best_model_finalized\")\n",
    "        save_model(best_model_finalized, \"best_blend_finalized\")\n",
    "        get_logs().to_csv(\"{}_{}_log.csv\".format(coin, return_period))\n",
    "  \n",
    "    return best_models, best_model_finalized, best_blend_finalized, get_logs()\n",
    "\n",
    "\n",
    "\n",
    "#=========================================================\n",
    "#====================CROSS-CORRELATION====================\n",
    "#=========================================================\n",
    "\n",
    "def cross_corr(crypto_wide, coin1, coin2, lag):\n",
    "    if lag > 0:\n",
    "        return(np.corrcoef(crypto_wide.dropna(subset=[coin1, coin2])[coin1].iloc[lag:], crypto_wide.dropna(subset=[coin1, coin2])[coin2].shift(lag).dropna())[0, 1])\n",
    "    elif lag < 0:\n",
    "        return(np.corrcoef(crypto_wide.dropna(subset=[coin1, coin2])[coin1].iloc[:lag], crypto_wide.dropna(subset=[coin1, coin2])[coin2].shift(lag).dropna())[0, 1])\n",
    "    elif lag == 0:\n",
    "        return(np.corrcoef(crypto_wide.dropna(subset=[coin1, coin2])[coin1], crypto_wide.dropna(subset=[coin1, coin2])[coin2])[0, 1])\n",
    "\n",
    "\n",
    "def cross_corr_range(crypto_wide, coin1, coin2, lag_range, plot=True):\n",
    "    result = pd.Series(lag_range).apply(lambda x: cross_corr(crypto_wide, coin1, coin2, int(round(x))))\n",
    "    result.index = lag_range\n",
    "    if plot:\n",
    "        plt.plot(result)\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_cross_corr_matrix(crypto_wide, price_cols, num_est = 10):\n",
    "    cross_corr_mat = pd.DataFrame([], columns=price_cols, index=price_cols)\n",
    "    print(\"Getting cross-correlations\")\n",
    "    print(\"\\t\", end=\"\")\n",
    "    for pair in tqdm(list(combinations(price_cols, 2))):\n",
    "        cross_corr_values = cross_corr_range(crypto_wide, pair[0], pair[1], np.linspace(-2880, 2880, num=num_est), plot=False)\n",
    "        greatest_result = cross_corr_values[cross_corr_values == cross_corr_values.max()]\n",
    "        greatest_lag = greatest_result.index[0]\n",
    "        greatest_corr = greatest_result.values[0]\n",
    "        if greatest_lag < 0:\n",
    "            cross_corr_mat.loc[pair[0], pair[1]] = greatest_corr\n",
    "        elif greatest_lag > 0:\n",
    "            cross_corr_mat.loc[pair[1], pair[0]] = greatest_corr\n",
    "        else:\n",
    "            pass\n",
    "    return cross_corr_mat.astype(float)\n",
    "\n",
    "\n",
    "def get_split_dates(crypto_wide, num_weeks_train, num_weeks_test, num_weeks_holdout):\n",
    "    # Defining our periods of interest\n",
    "    holdout_start = crypto_wide.index.max() - pd.Timedelta(weeks=num_weeks_holdout)\n",
    "    test_start = holdout_start - pd.Timedelta(weeks=num_weeks_test)\n",
    "    train_start = test_start - pd.Timedelta(weeks=num_weeks_train)\n",
    "    print(\"Number of training samples: {}\".format(((crypto_wide.index < test_start) & (crypto_wide.index >= train_start)).sum()))\n",
    "    print(\"Number of test samples: {}\".format(((crypto_wide.index >= test_start) & (crypto_wide.index < holdout_start)).sum()))\n",
    "    print(\"Number of holdout samples: {}\".format(((crypto_wide.index >= holdout_start)).sum()))\n",
    "    return train_start, test_start, holdout_start\n",
    "\n",
    "\n",
    "def prepare_data(crypto_wide, coin_of_interest, columns_of_interest, train_start, test_start, holdout_start, cluster_num=None, \n",
    "                  cross_correlation_matrix=None, cross_corr_thresh=0.50, outlier_column=False):\n",
    "    all_coins = [coin_of_interest]\n",
    "\n",
    "    # Yielding our testing/training/holdout dataframes\n",
    "    crypto_train = crypto_wide.loc[train_start:test_start]\n",
    "    crypto_test = crypto_wide.loc[test_start:holdout_start]\n",
    "    crypto_holdout = crypto_wide.loc[holdout_start:]\n",
    "    crypto_test_holdout = pd.concat([crypto_test, crypto_holdout])\n",
    "\n",
    "    # Getting clusters for each coin return\n",
    "    print(\"Fitting clusters on training data\")\n",
    "    print(\"\\t\", end=\"\")\n",
    "    cluster_cols = [col for col in crypto_train if \"{}_pct_change\".format(coin_of_interest) in col]\n",
    "    crypto_return_clusters_train, cluster_model_dict, order_dict = clusterize(crypto_wide.loc[train_start:test_start, cluster_cols], cluster_num=cluster_num)\n",
    "#     crypto_return_clusters_train = crypto_return_clusters_train.loc[train_start:]\n",
    "    crypto_return_clusters_test_holdout = crypto_test_holdout[cluster_cols].copy()\n",
    "\n",
    "    print(\"Predicting clusters on test and holdout data\")\n",
    "    print(\"\\t\", end=\"\")\n",
    "    for col in tqdm(cluster_cols):\n",
    "        kmean_labels = cluster_model_dict[col].predict(crypto_return_clusters_test_holdout[col].values.reshape(-1, 1))\n",
    "        crypto_return_clusters_test_holdout[col] = list(map(lambda x: order_dict[col][x], kmean_labels))\n",
    "    \n",
    "    crypto_return_clusters_train = crypto_return_clusters_train.astype(int).astype(str)\n",
    "    crypto_return_clusters_test_holdout = crypto_return_clusters_test_holdout.astype(int).astype(str)\n",
    "    crypto_train = crypto_train.join(crypto_return_clusters_train, lsuffix=\"\", rsuffix=\"_clust\")\n",
    "    crypto_test_holdout = crypto_test_holdout.join(crypto_return_clusters_test_holdout, lsuffix=\"\", rsuffix=\"_clust\")\n",
    "    columns_of_interest += [col for col in crypto_train if \"clust\" in col]\n",
    "  \n",
    "    # Adding coin data for potentially causal coins\n",
    "    if cross_correlation_matrix is not None:\n",
    "        print(\"Getting additional potentially influential coins\")\n",
    "        print(\"\\t\", end=\"\")\n",
    "        additional_coins = (cross_correlation_matrix.abs() > cross_corr_thresh).query(coin_of_interest).index.tolist()\n",
    "        all_coins += additional_coins\n",
    "        additional_cols = []\n",
    "        for coin in tqdm(additional_coins):\n",
    "            additional_cols += [col for col in crypto_train if coin in col]\n",
    "        columns_of_interest += additional_cols\n",
    "        columns_of_interest = list(set(columns_of_interest))\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "    # Getting coin-based outliers\n",
    "    if outlier_column:\n",
    "        print(\"Getting outliers\")\n",
    "        print(\"\\t\", end=\"\")\n",
    "        for coin in tqdm(all_coins):\n",
    "            # Getting coin-related columns to create outlier computation\n",
    "            coin_cols = [col for col in crypto_wide if coin in col]\n",
    "\n",
    "            # training the model\n",
    "            clf = IsolationForest(max_samples=100, random_state=1)\n",
    "            clf.fit(crypto_train[coin_cols].dropna().values)\n",
    "            train_outliers = clf.predict(crypto_train[coin_cols].dropna().values)\n",
    "            outlier_col = \"{}_OUTLIER\".format(coin)\n",
    "            columns_of_interest += [outlier_col]\n",
    "            crypto_train[outlier_col] = np.nan\n",
    "            crypto_train.loc[~crypto_train[coin_cols].isna().any(axis=1), outlier_col] = train_outliers.astype(int).astype(str)\n",
    "\n",
    "            # predicting model on test/holdout partitions\n",
    "            test_outliers = clf.predict(crypto_test_holdout[coin_cols].dropna().values)\n",
    "            crypto_test_holdout[outlier_col] = np.nan\n",
    "            crypto_test_holdout.loc[~crypto_test_holdout[coin_cols].isna().any(axis=1), outlier_col] = test_outliers.astype(int).astype(str)\n",
    "  \n",
    "    # Selecting only our columns of interest\n",
    "    crypto_train = crypto_train[columns_of_interest]\n",
    "    crypto_test_holdout = crypto_test_holdout[columns_of_interest]\n",
    "    return crypto_train, crypto_test_holdout, crypto_return_clusters_train, crypto_return_clusters_test_holdout\n",
    "\n",
    "\n",
    "def add_target(coin_of_interest, crypto_train, crypto_test_holdout, return_period, test_start):\n",
    "    # Joining on clusters of our coin returns\n",
    "    target = \"{}_pct_change_{}\".format(coin_of_interest, return_period)\n",
    "    combined_df = pd.concat([crypto_train, crypto_test_holdout])\n",
    "    combined_cluster_df = pd.concat([crypto_return_clusters_train, crypto_return_clusters_test_holdout])\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
    "    combined_cluster_df = combined_cluster_df[~combined_cluster_df.index.duplicated(keep='first')]\n",
    "    combined_df[\"target\"] = combined_cluster_df[target].shift(-return_period)\n",
    "    crypto_train, crypto_test_holdout = combined_df.loc[:test_start], combined_df.loc[test_start:]\n",
    "    cluster_desc = crypto_train.groupby(crypto_return_clusters_train[target])[[target]].describe()\n",
    "    print(\"Unique values of target: {}\".format(crypto_train[\"target\"].nunique()))\n",
    "    return crypto_train, crypto_test_holdout, cluster_desc\n",
    "\n",
    "\n",
    "def split_df(crypto_train, crypto_test_holdout, holdout_start):\n",
    "    # Further partitioning our combined test/holdout dataframe\n",
    "    crypto_test, crypto_holdout = crypto_test_holdout.loc[:holdout_start].dropna(subset=[\"target\"]), crypto_test_holdout.loc[holdout_start:].dropna(subset=[\"target\"])\n",
    "\n",
    "    # Converting target to string for classification task\n",
    "    crypto_train[\"target\"] = crypto_train[\"target\"].astype(int).astype(str)\n",
    "    crypto_test[\"target\"] = crypto_test[\"target\"].astype(int).astype(str)\n",
    "    crypto_test[\"target\"] = crypto_test[\"target\"].astype(int).astype(str)\n",
    "    return crypto_train, crypto_test, crypto_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: ········\n",
      "password: ········\n"
     ]
    }
   ],
   "source": [
    "username = getpass(\"username: \")\n",
    "password = getpass(\"password: \")\n",
    "\n",
    "# Reading in data from snowflake\n",
    "conn  = snowflake.connector.connect(user=username,\n",
    "                                   password=password,\n",
    "                                   account=\"kga72450.us-east-1\")\n",
    "\n",
    "conn.cursor().execute(\"USE WAREHOUSE COMPUTE_WH\")\n",
    "conn.cursor().execute(\"USE DATABASE CRYPTO\")\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "sql = \"select * from TOP_CRYPTO_YTD\"\n",
    "cur.execute(sql)\n",
    "crypto_df = cur.fetch_pandas_all()\n",
    "\n",
    "# Converting types\n",
    "crypto_df = crypto_df.astype({col:float for col in crypto_df.columns if col not in [\"NA\", \"COIN\"]})\n",
    "\n",
    "# Converting timestamps to date-time\n",
    "convert_time = lambda x: datetime.fromtimestamp(x/1000)# Convert from millisecond to second\n",
    "crypto_df[\"OPEN_TIME\"] = crypto_df[\"OPEN_TIME\"].apply(convert_time)\n",
    "crypto_df[\"CLOSE TIME\"] = crypto_df[\"CLOSE TIME\"].apply(convert_time)\n",
    "\n",
    "# Pivoting columns to wide\n",
    "crypto_open = crypto_df.pivot_table(columns=[\"COIN\"], index=[\"OPEN_TIME\"], values=[\"OPEN\"])[\"OPEN\"]\n",
    "crypto_volume = crypto_df.pivot_table(columns=[\"COIN\"], index=[\"OPEN_TIME\"], values=[\"VOLUME\"])[\"VOLUME\"]\n",
    "crypto_num_trade = crypto_df.pivot_table(columns=[\"COIN\"], index=[\"OPEN_TIME\"], values=[\"NUMBER_OF_TRADES\"])[\"NUMBER_OF_TRADES\"]\n",
    "\n",
    "crypto_seed_df = crypto_open.join(crypto_volume.join(crypto_num_trade, lsuffix=\"_VOLUME\", rsuffix=\"_NUM_TRADES\"))# Joining everything together\n",
    "volume_cols = [col for col in crypto_seed_df if \"VOLUME\" in col]\n",
    "num_trades_cols = [col for col in crypto_seed_df if \"NUM_TRADES\" in col]\n",
    "price_cols = [col for col in crypto_seed_df if col.endswith(\"USDT\")]\n",
    "\n",
    "# Getting data with all the indices for analysis\n",
    "crypto_wide = get_all_indices(crypto_seed_df, rolling_periods=[1, 5, 10, 60, 180], groupby_col=None, metrics=[\"pct_change\", \"diff\"], \n",
    "                              col_dict={\"pct_change\":price_cols, \"diff\":(num_trades_cols + volume_cols)})\n",
    "crypto_wide = crypto_wide.reset_index().rename(columns={\"OPEN_TIME\":\"Time\"}).set_index(\"Time\")\n",
    "\n",
    "del crypto_seed_df, crypto_num_trade, crypto_volume, crypto_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 40320\n",
      "Number of test samples: 10080\n",
      "Number of holdout samples: 10081\n",
      "Getting cross-correlations\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [05:14<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting clusters on training data\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:31<00:00, 30.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting clusters on test and holdout data\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting additional potentially influential coins\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 6232.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting outliers\n",
      "\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:11<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of target: 3\n"
     ]
    }
   ],
   "source": [
    "num_weeks_train, num_weeks_test, num_weeks_holdout = 4, 1, 1\n",
    "train_start, test_start, holdout_start = get_split_dates(crypto_wide, num_weeks_train, num_weeks_test, num_weeks_holdout)\n",
    "cross_corr_mat = get_cross_corr_matrix(crypto_wide.loc[train_start:test_start], price_cols, num_est=10)\n",
    "\n",
    "for coin_of_interest in set(price_cols):\n",
    "    crypto_train, crypto_test_holdout, crypto_return_clusters_train, crypto_return_clusters_test_holdout = prepare_data(crypto_wide, coin_of_interest, [col for col in crypto_wide if (\"BTCUSDT\" in col) or (\"ETHUSDT\" in col) or (coin_of_interest in col)], train_start=train_start, test_start=test_start, holdout_start=holdout_start, cluster_num=None, \n",
    "                      cross_correlation_matrix=cross_corr_mat, cross_corr_thresh=0.50, outlier_column=True)\n",
    "    return_periods = [5]\n",
    "#     return_periods = [1, 5, 10, 30, 60, 180]\n",
    "    test_args = {\"fold_strategy\":\"timeseries\", \"create_clusters\":True}\n",
    "\n",
    "    for return_period in return_periods:\n",
    "        crypto_train_tmp, crypto_test_holdout_tmp, cluster_desc = add_target(coin_of_interest, crypto_train, crypto_test_holdout, return_period, test_start)\n",
    "        crypto_train_tmp, crypto_test_tmp, crypto_holdout_tmp = split_df(crypto_train_tmp, crypto_test_holdout_tmp, holdout_start)\n",
    "        best_models, best_model_finalized, best_blend_finalized, logs = pycaret_automl(crypto_train_tmp, crypto_test_tmp, coin_of_interest, return_period, plot=False, download=False, save_folder=\"./crypto_models/{}/{}\".format(coin_of_interest, return_period),\n",
    "                                                                                      pycaret_args=test_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
